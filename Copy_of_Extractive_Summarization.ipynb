{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RG39tsRHTdh",
        "outputId": "b30deb6a-1997-443d-8546-4e0f08a186e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-learn-intelex\n",
            "  Downloading scikit_learn_intelex-2023.0.1-py39-none-manylinux1_x86_64.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.9/dist-packages (from scikit-learn-intelex) (1.2.2)\n",
            "Collecting daal4py==2023.0.1\n",
            "  Downloading daal4py-2023.0.1-py39-none-manylinux1_x86_64.whl (12.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting daal==2023.0.1\n",
            "  Downloading daal-2023.0.1-py2.py3-none-manylinux1_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.9/dist-packages (from daal4py==2023.0.1->scikit-learn-intelex) (1.22.4)\n",
            "Collecting tbb==2021.*\n",
            "  Downloading tbb-2021.8.0-py2.py3-none-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22->scikit-learn-intelex) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22->scikit-learn-intelex) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22->scikit-learn-intelex) (1.1.1)\n",
            "Installing collected packages: tbb, daal, daal4py, scikit-learn-intelex\n",
            "Successfully installed daal-2023.0.1 daal4py-2023.0.1 scikit-learn-intelex-2023.0.1 tbb-2021.8.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install scikit-learn-intelex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsrzpuzKJh-v",
        "outputId": "046d5d16-d992-4212-e899-d7fd45fd0971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
          ]
        }
      ],
      "source": [
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITw6QWCmQGL8",
        "outputId": "b15a173b-23e0-4eac-8894-80862139e1a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# --- Mounting drive --- \n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRdsQ98lQqoD"
      },
      "outputs": [],
      "source": [
        "# --- tqdm version 4.36.1 is required --- \n",
        "\n",
        "\n",
        "!pip install tqdm==4.36.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI8aTY50JMpe"
      },
      "outputs": [],
      "source": [
        "!pip install rogue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--cFi0bdP4js"
      },
      "outputs": [],
      "source": [
        "# --- Import essential packages --- \n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "from sklearn.datasets import load_files\n",
        "import glob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1CYgbmFP4nx",
        "outputId": "8f8f550f-26ba-4722-a0fe-31aa377f73e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
            "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
          ]
        }
      ],
      "source": [
        "# --- Implementing pretrained word embeddings --- \n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()\n",
        "\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "\n",
        "glove_file = datapath(\"/content/drive/MyDrive/hackathon/test.txt\")\n",
        "tmp_file = get_tmpfile(\"/content/drive/MyDrive/hackathon/temp.txt\")\n",
        "converted_file = glove2word2vec(glove_file, tmp_file) \n",
        "\n",
        "\n",
        "# Source: https://radimrehurek.com/gensim/scripts/glove2word2vec.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEyjwJkxP4n8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d7b7322-40df-49f9-84fa-ad3311471bb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
          ]
        }
      ],
      "source": [
        "# --- Loading the Glove embeddings in word2vec format ---\n",
        "\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGWXe7cUP4oA",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "glove_model[\"the\"]  # Checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t6QzSa9UP1j"
      },
      "outputs": [],
      "source": [
        "# --- Stop word removal function --- \n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "def stop_word_remove(sentence):\n",
        "    temp = [token for token in sentence.split() if token not in STOP_WORDS]\n",
        "    return ' '.join(word for word in temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vFE3uAfZu14"
      },
      "outputs": [],
      "source": [
        "# --- Function to read the papers from their paths --- \n",
        "\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()\n",
        "def read_paper(path):\n",
        "  f = open(path, 'r', encoding=\"utf-8\")\n",
        "  text = str(f.read())\n",
        "  f.close()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFGEdxXfaBmA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b80b807-8c29-42be-a142-48fb8e45245b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
          ]
        }
      ],
      "source": [
        "# --- Function to preprocess the papers --- \n",
        "\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()\n",
        "def process_paper(text):\n",
        "\n",
        "  # Removes unwanted characters, accounting for unicode characters\n",
        "  text = re.sub(\"@&#\", \" \", text)\n",
        "  text = re.sub(\"\\n\", \" \", text)\n",
        "  text = (text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
        "\n",
        "  # Extracting the highlights, body from the paper\n",
        "  highlights = re.findall(r'HIGHLIGHTS(.*?)KEYPHRASES', text,  flags = re.I)[0]\n",
        "  body_main = re.findall(r'.*(?:abstract)(.*?)', text, flags=re.I)[0]\n",
        "\n",
        "  # Making a copy of the body, lowercasing body text, removing punctuations & extra spaces\n",
        "  dummy_body = body_main.lower()\n",
        "  dummy_body = re.sub('[^\\w\\s\\d\\.]','',dummy_body)\n",
        "  dummy_body = ' '.join(dummy_body.split())\n",
        "  dummy_body = dummy_body.split(\".\")\n",
        "\n",
        "  # Removing extra spaces from the body text, which will be preserved to produce summaries\n",
        "  # And splitting into sentences\n",
        "  body = ' '.join(body_main.split())\n",
        "  body = body.split(\".\")\n",
        "\n",
        "  # Removing sentences that are too short or too long, as they wouldn't make apt summary text\n",
        "  for i,x in enumerate(dummy_body):\n",
        "    if (len(x.split())) < 3 or (len(x.split())) > 15: \n",
        "        dummy_body.pop(i)\n",
        "        body.pop(i)\n",
        "\n",
        "  # Making a copy of the highlights, lowercasing body text, removing punctuations & extra spaces\n",
        "  dummy_highlights = highlights.lower()\n",
        "  dummy_highlights = re.sub('[^\\w\\s\\d]','',dummy_highlights)\n",
        "  dummy_highlights = ' '.join(dummy_highlights.split())\n",
        "\n",
        "  # Removing stop words from body & highlights\n",
        "  body_copy = []\n",
        "  for x in dummy_body:\n",
        "    body_copy.append(stop_word_remove(x))\n",
        "  highlight_copy = []\n",
        "  for x in dummy_highlights.split():\n",
        "      highlight_copy.append(stop_word_remove(x))\n",
        "\n",
        "  \n",
        "  # Combing all of the highlights into one string    \n",
        "  highlight_copy = \" \".join(sentence for sentence in highlight_copy)\n",
        "  highlight_copy = \" \".join(highlight_copy.split())\n",
        "\n",
        "  return body_main, body_copy, highlights, highlight_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hesoGRNOdCCR"
      },
      "outputs": [],
      "source": [
        "from scipy.special import expit\n",
        "\n",
        "\n",
        "# Function to calculate sentence Score\n",
        "def document_score(body_copy, highlight_copy):\n",
        "  # Getting word vectors for the body\n",
        "  body_vectors = []\n",
        "  for sent in body_copy:\n",
        "      sent_vec = []\n",
        "      for word in sent.split():\n",
        "          try:\n",
        "              sent_vec.append(glove_model[word])\n",
        "          # If the word vector isn't there in the model\n",
        "          # then use the vector of the word \"Visual\"\n",
        "          except:\n",
        "              sent_vec.append(glove_model[\"visual\"])\n",
        "      body_vectors.append(sent_vec)\n",
        "\n",
        "  # Getting word vectors for the highlights\n",
        "  highlight_vectors = []\n",
        "  for word in highlight_copy.split():\n",
        "      try:\n",
        "          highlight_vectors.append(glove_model[word])\n",
        "      except:\n",
        "          highlight_vectors.append(glove_model[\"visual\"])\n",
        "\n",
        "  # Finding the rouge score for each sentence by counting the # of common words\n",
        "  # & dividing by length of sentence\n",
        "  doc_score = []\n",
        "  for sent in body_vectors:\n",
        "      sent_score = 0\n",
        "      for word in sent:\n",
        "          for w in highlight_vectors:\n",
        "              if (word == w).all():\n",
        "                  sent_score+=1\n",
        "      try: \n",
        "        doc_score.append(expit(sent_score/len(sent)))\n",
        "      except:\n",
        "        doc_score.append(0)\n",
        "  return doc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OterM5aVd35B"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "from gensim.models import doc2vec\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "# Function to create document vectors\n",
        "def create_document_vector(body_main, doc_score):\n",
        "  # Load data\n",
        "  doc1 = [body_main]\n",
        "\n",
        "  # Transforming data\n",
        "  docs = []\n",
        "  analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
        "  for i, d in enumerate(doc1):\n",
        "      words = d.lower().split()\n",
        "      tags = [i]\n",
        "      docs.append(analyzedDocument(words, tags))\n",
        "\n",
        "  # Training model\n",
        "  model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)\n",
        "\n",
        "  # Getting vectors\n",
        "  doc_vec = model.docvecs[0]\n",
        "\n",
        "  doc_vectors = []\n",
        "  for i in range(len(doc_score)):\n",
        "    doc_vectors.append(doc_vec)\n",
        "  return doc_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVE8Pu39f8sh"
      },
      "outputs": [],
      "source": [
        "# Function to create datasets\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()\n",
        "def create_data(path):\n",
        "  text = read_paper(path)\n",
        "  body_main, body_copy, highlights, highlight_copy = process_paper(text)\n",
        "  doc_score = document_score(body_copy, highlight_copy)\n",
        "  doc_vectors = create_document_vector(body_main, doc_score)\n",
        "  sent_vectors = create_sentence_vectors(body_copy)\n",
        "  x = np.concatenate([doc_vectors, sent_vectors], axis=1).tolist()\n",
        "  x = pd.DataFrame(x)\n",
        "  y = pd.DataFrame(doc_score)\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYe9eKbZg2il"
      },
      "outputs": [],
      "source": [
        "# --- Suppress all warnings --- \n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Creating a list of all file paths & partially fitting the model --- \n",
        "\n",
        "paths = glob.glob(\"/content/drive/My Drive/NLP Project/Project Final/Parsed_Papers/*.txt\")\n",
        "for i,path in enumerate(tqdm(paths[0:20])):\n",
        "  x, y = create_data(path)\n",
        "  Model.partial_fit(x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y1KLLcuP4lr"
      },
      "outputs": [],
      "source": [
        "f = open(\"/content/drive/MyDrive/hackathon/test.txt\", 'r', encoding=\"utf-8\")\n",
        "text = str(f.read())\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()"
      ],
      "metadata": {
        "id": "BrvgAf5cOqU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e6586c-c23a-45fb-829e-90b44a485899"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgnXc9NIcyIF"
      },
      "outputs": [],
      "source": [
        "text = re.sub(\"@&#\", \" \", text)\n",
        "text = re.sub(\"\\n\", \" \", text)\n",
        "text = (text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDosqrG8P4mK"
      },
      "outputs": [],
      "source": [
        "main_title = re.findall(r'MAIN-TITLE(.*?)HIGHLIGHTS', text, flags = re.I)\n",
        "main_title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JGsqD7TP4mR"
      },
      "outputs": [],
      "source": [
        "highlights = re.findall(r'HIGHLIGHTS(.*?)KEYPHRASES', text,  flags = re.I)[0]\n",
        "highlights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWhThlrNP4mc"
      },
      "outputs": [],
      "source": [
        "keyphrases = re.findall(r'.*keyphrases(.*?)(?:introduction|abstract).*', text, flags = re.I)\n",
        "keyphrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZISq17HGP4mh"
      },
      "outputs": [],
      "source": [
        "body_main = re.findall(r'.*(?:abstract)(.*?)references', text, flags=re.I)[0]\n",
        "body_main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIK6aj99e0xE"
      },
      "outputs": [],
      "source": [
        "dummy_body = body_main.lower()\n",
        "dummy_body = re.sub('[^\\w\\s\\d\\.]','',dummy_body)\n",
        "dummy_body = ' '.join(dummy_body.split())\n",
        "dummy_body = dummy_body.split(\".\")\n",
        "dummy_body[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ThCpCdsP4mj"
      },
      "outputs": [],
      "source": [
        "body = ' '.join(body_main.split())\n",
        "body = body.split(\".\")\n",
        "body[:5]\n",
        "\n",
        "# for x in body:\n",
        "#     if x == \" \":\n",
        "#         body.remove(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd7YGJmolQmg"
      },
      "outputs": [],
      "source": [
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()\n",
        "for i,x in enumerate(dummy_body):\n",
        "  if (len(x.split())) < 3 or (len(x.split())) > 15: \n",
        "      dummy_body.pop(i)\n",
        "      body.pop(i)\n",
        "print(len(dummy_body),len(body))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5VpabB2bC3h"
      },
      "outputs": [],
      "source": [
        "dummy_highlights = highlights.lower()\n",
        "dummy_highlights = re.sub('[^\\w\\s\\d]','',dummy_highlights)\n",
        "dummy_highlights = ' '.join(dummy_highlights.split())\n",
        "dummy_highlights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW4EZc7MP4nP"
      },
      "outputs": [],
      "source": [
        "body_copy = []\n",
        "\n",
        "for x in dummy_body:\n",
        "    body_copy.append(stop_word_remove(x))\n",
        "        \n",
        "body_copy[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08vyXxsoP4na"
      },
      "outputs": [],
      "source": [
        "highlight_copy = []\n",
        "\n",
        "for x in dummy_highlights.split():\n",
        "    highlight_copy.append(stop_word_remove(x))\n",
        "    \n",
        "highlight_copy = \" \".join(sentence for sentence in highlight_copy)\n",
        "highlight_copy = \" \".join(highlight_copy.split())\n",
        "highlight_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRCs5aCCv611"
      },
      "outputs": [],
      "source": [
        "body_vectors = []\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()\n",
        "\n",
        "for sent in body_copy:\n",
        "    sent_vec = []\n",
        "    for word in sent.split():\n",
        "        try:\n",
        "            sent_vec.append(glove_model[word])\n",
        "        except:\n",
        "            sent_vec.append(glove_model[\"visual\"])\n",
        "    body_vectors.append(sent_vec)\n",
        "    \n",
        "    \n",
        "body_vectors[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoAEJlNHv76p"
      },
      "outputs": [],
      "source": [
        "highlight_vectors = []\n",
        "\n",
        "\n",
        "for word in highlight_copy.split():\n",
        "    try:\n",
        "        highlight_vectors.append(glove_model[word])\n",
        "    except:\n",
        "        highlight_vectors.append(glove_model[\"visual\"])\n",
        "\n",
        "\n",
        "highlight_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_g22TYwYwEL4"
      },
      "outputs": [],
      "source": [
        "from scipy.special import expit\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()\n",
        "\n",
        "doc_score = []\n",
        "\n",
        "for sent in body_vectors:\n",
        "\n",
        "    sent_score = 0\n",
        "\n",
        "    for word in sent:\n",
        "        for w in highlight_vectors:\n",
        "            if (word == w).all():\n",
        "                sent_score+=1\n",
        "    try: \n",
        "      doc_score.append(expit(sent_score/len(sent)))\n",
        "    except:\n",
        "      doc_score.append(0)\n",
        "\n",
        "      \n",
        "y = doc_score\n",
        "y[:10]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "oldHeight": 122.85,
      "position": {
        "height": "40px",
        "left": "1266px",
        "right": "20px",
        "top": "120px",
        "width": "250px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "varInspector_section_display": "none",
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}